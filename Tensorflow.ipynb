{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes),'uint8')\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                species   margin1   margin2   margin3   margin4  \\\n",
       "0   1            Acer_Opalus  0.007812  0.023438  0.023438  0.003906   \n",
       "1   2  Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625   \n",
       "2   3   Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812   \n",
       "3   5        Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859   \n",
       "4   6     Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766   \n",
       "\n",
       "    margin5   margin6   margin7  margin8    ...      texture55  texture56  \\\n",
       "0  0.011719  0.009766  0.027344      0.0    ...       0.007812   0.000000   \n",
       "1  0.025391  0.001953  0.019531      0.0    ...       0.000977   0.000000   \n",
       "2  0.003906  0.005859  0.068359      0.0    ...       0.154300   0.000000   \n",
       "3  0.021484  0.019531  0.023438      0.0    ...       0.000000   0.000977   \n",
       "4  0.013672  0.015625  0.005859      0.0    ...       0.096680   0.000000   \n",
       "\n",
       "   texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "0   0.002930   0.002930   0.035156        0.0        0.0   0.004883   \n",
       "1   0.000000   0.000977   0.023438        0.0        0.0   0.000977   \n",
       "2   0.005859   0.000977   0.007812        0.0        0.0   0.000000   \n",
       "3   0.000000   0.000000   0.020508        0.0        0.0   0.017578   \n",
       "4   0.021484   0.000000   0.000000        0.0        0.0   0.000000   \n",
       "\n",
       "   texture63  texture64  \n",
       "0   0.000000   0.025391  \n",
       "1   0.039062   0.022461  \n",
       "2   0.020508   0.002930  \n",
       "3   0.000000   0.047852  \n",
       "4   0.000000   0.031250  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = LabelEncoder().fit(df_train['species'])\n",
    "y_train = labels.transform(df_train['species'])\n",
    "#y_train = onehot(y_train,len(np.unique(y_train)))\n",
    "x_train = np.array(df_train.drop(['id','species'],axis=1))\n",
    "x_test = np.array(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(x_train, y_train,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_train))\n",
    "n_features = x_train.shape[1]\n",
    "\n",
    "n_nodes_hl1 = 200\n",
    "n_nodes_hl2 = 200\n",
    "\n",
    "batch_size = 100 # batches of features feed to network\n",
    "\n",
    "x = tf.placeholder('float', [None, n_features])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([n_features, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    " \t# (input_data * weights) + biases\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    output = tf.matmul(l2,output_layer['weights']) + output_layer['biases']\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "    #cost = tf.reduce_mean( -tf.reduce_mean(tf.log(prediction)*y, reduction_indices=[1]) )\n",
    "    # optimizer has a default parameter- learning rate = 0.01\n",
    "    ##optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(0.1).minimize(cost)\n",
    "    \n",
    "    hm_epochs = 500 #cycles feedforward+backward\n",
    "    train = []\n",
    "    valid = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(len(train_x)//batch_size):\n",
    "                idx = range(i*batch_size, (i+1)*batch_size)\n",
    "                epoch_x = train_x[idx]\n",
    "                epoch_y = train_y[idx]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x:epoch_x, y:onehot(epoch_y,n_classes)})\n",
    "                epoch_loss += c\n",
    "            \n",
    "            train_accu = accuracy.eval({x:train_x, y:onehot(train_y,n_classes)})\n",
    "            valid_accu = accuracy.eval({x:valid_x, y:onehot(valid_y,n_classes)})\n",
    "            train += [train_accu]\n",
    "            valid += [valid_accu]    \n",
    "            print ('Epoch', epoch+1,'loss:',epoch_loss,'train_accu:',train_accu,'valid_accu:',valid_accu)\n",
    "            \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(len(train)),train,'r',valid,'b')\n",
    "        plt.legend(['train','valid'])\n",
    "        #test_acc = accuracy.eval({x:x_test, y:y_test})\n",
    "        #print('test_acc:',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1486.87451172 train_accu: 0.0164141 valid_accu: 0.0151515\n",
      "Epoch 2 loss: 325.784936905 train_accu: 0.0984849 valid_accu: 0.0909091\n",
      "Epoch 3 loss: 49.2350921631 train_accu: 0.135101 valid_accu: 0.121212\n",
      "Epoch 4 loss: 27.4041395187 train_accu: 0.267677 valid_accu: 0.156566\n",
      "Epoch 5 loss: 19.7192900181 train_accu: 0.416667 valid_accu: 0.262626\n",
      "Epoch 6 loss: 13.7940909863 train_accu: 0.569444 valid_accu: 0.393939\n",
      "Epoch 7 loss: 9.0396464467 train_accu: 0.727273 valid_accu: 0.510101\n",
      "Epoch 8 loss: 5.59688782692 train_accu: 0.834596 valid_accu: 0.580808\n",
      "Epoch 9 loss: 3.36358323693 train_accu: 0.875 valid_accu: 0.621212\n",
      "Epoch 10 loss: 2.0758883357 train_accu: 0.924242 valid_accu: 0.651515\n",
      "Epoch 11 loss: 1.32477554679 train_accu: 0.945707 valid_accu: 0.656566\n",
      "Epoch 12 loss: 0.766810402274 train_accu: 0.954545 valid_accu: 0.671717\n",
      "Epoch 13 loss: 0.529178153723 train_accu: 0.965909 valid_accu: 0.676768\n",
      "Epoch 14 loss: 0.304568633437 train_accu: 0.968434 valid_accu: 0.69697\n",
      "Epoch 15 loss: 0.210534544662 train_accu: 0.972222 valid_accu: 0.722222\n",
      "Epoch 16 loss: 0.167556761764 train_accu: 0.97096 valid_accu: 0.712121\n",
      "Epoch 17 loss: 0.129394058138 train_accu: 0.977273 valid_accu: 0.722222\n",
      "Epoch 18 loss: 0.0762967793271 train_accu: 0.973485 valid_accu: 0.722222\n",
      "Epoch 19 loss: 0.0548295341432 train_accu: 0.973485 valid_accu: 0.727273\n",
      "Epoch 20 loss: 0.0387990106829 train_accu: 0.972222 valid_accu: 0.70202\n",
      "Epoch 21 loss: 0.0293953302316 train_accu: 0.972222 valid_accu: 0.712121\n",
      "Epoch 22 loss: 0.0243557393551 train_accu: 0.972222 valid_accu: 0.717172\n",
      "Epoch 23 loss: 0.0211644575465 train_accu: 0.972222 valid_accu: 0.712121\n",
      "Epoch 24 loss: 0.0183462772984 train_accu: 0.972222 valid_accu: 0.712121\n",
      "Epoch 25 loss: 0.0167759790784 train_accu: 0.97096 valid_accu: 0.712121\n",
      "Epoch 26 loss: 0.015248418902 train_accu: 0.969697 valid_accu: 0.707071\n",
      "Epoch 27 loss: 0.0140039670514 train_accu: 0.969697 valid_accu: 0.707071\n",
      "Epoch 28 loss: 0.0129622556269 train_accu: 0.97096 valid_accu: 0.707071\n",
      "Epoch 29 loss: 0.0120504084043 train_accu: 0.973485 valid_accu: 0.712121\n",
      "Epoch 30 loss: 0.0111795647535 train_accu: 0.972222 valid_accu: 0.722222\n",
      "Epoch 31 loss: 0.0103578872513 train_accu: 0.972222 valid_accu: 0.717172\n",
      "Epoch 32 loss: 0.0097115780809 train_accu: 0.972222 valid_accu: 0.717172\n",
      "Epoch 33 loss: 0.00908423727378 train_accu: 0.973485 valid_accu: 0.727273\n",
      "Epoch 34 loss: 0.00858692638576 train_accu: 0.973485 valid_accu: 0.727273\n",
      "Epoch 35 loss: 0.00812495331047 train_accu: 0.973485 valid_accu: 0.727273\n",
      "Epoch 36 loss: 0.00769774516812 train_accu: 0.973485 valid_accu: 0.732323\n",
      "Epoch 37 loss: 0.00730909639969 train_accu: 0.973485 valid_accu: 0.732323\n",
      "Epoch 38 loss: 0.0069435738842 train_accu: 0.974747 valid_accu: 0.732323\n",
      "Epoch 39 loss: 0.00663045339752 train_accu: 0.974747 valid_accu: 0.732323\n",
      "Epoch 40 loss: 0.00634887919296 train_accu: 0.974747 valid_accu: 0.732323\n",
      "Epoch 41 loss: 0.0060665695928 train_accu: 0.974747 valid_accu: 0.732323\n",
      "Epoch 42 loss: 0.00582146761008 train_accu: 0.974747 valid_accu: 0.727273\n",
      "Epoch 43 loss: 0.00559085444547 train_accu: 0.974747 valid_accu: 0.722222\n",
      "Epoch 44 loss: 0.00536990221008 train_accu: 0.974747 valid_accu: 0.722222\n",
      "Epoch 45 loss: 0.00515900983009 train_accu: 0.97601 valid_accu: 0.722222\n",
      "Epoch 46 loss: 0.00498480262468 train_accu: 0.97601 valid_accu: 0.722222\n",
      "Epoch 47 loss: 0.00478847351042 train_accu: 0.97601 valid_accu: 0.722222\n",
      "Epoch 48 loss: 0.00461392189027 train_accu: 0.97601 valid_accu: 0.722222\n",
      "Epoch 49 loss: 0.00444855855312 train_accu: 0.97601 valid_accu: 0.722222\n",
      "Epoch 50 loss: 0.00429610643187 train_accu: 0.97601 valid_accu: 0.727273\n",
      "Epoch 51 loss: 0.00414942676434 train_accu: 0.977273 valid_accu: 0.727273\n",
      "Epoch 52 loss: 0.00401768108713 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 53 loss: 0.00388723050128 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 54 loss: 0.00376296279137 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 55 loss: 0.00364204370999 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 56 loss: 0.00352907204069 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 57 loss: 0.00342606191407 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 58 loss: 0.00332794437418 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 59 loss: 0.00323141578701 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 60 loss: 0.00312894492527 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 61 loss: 0.00303675155737 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 62 loss: 0.00294458330609 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 63 loss: 0.00285790569615 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 64 loss: 0.00278118086862 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 65 loss: 0.00270627075224 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 66 loss: 0.00263169279788 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 67 loss: 0.00255909762927 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 68 loss: 0.00249208208697 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 69 loss: 0.00242612721922 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 70 loss: 0.00236286781728 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 71 loss: 0.00230064430798 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 72 loss: 0.00224332620564 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 73 loss: 0.00218619186489 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 74 loss: 0.00213258677104 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 75 loss: 0.00208251186996 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 76 loss: 0.0020344122313 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 77 loss: 0.00198663830815 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 78 loss: 0.00194136812934 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 79 loss: 0.00189543519809 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 80 loss: 0.00185499018698 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 81 loss: 0.00181351994979 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 82 loss: 0.00177417353552 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 83 loss: 0.00173652933154 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 84 loss: 0.0016999469517 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 85 loss: 0.00166442913178 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 86 loss: 0.00163056816382 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 87 loss: 0.00159761837858 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 88 loss: 0.00156665206305 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 89 loss: 0.00153529121599 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 90 loss: 0.00150473110261 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 91 loss: 0.00147498314618 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 92 loss: 0.00144655584882 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 93 loss: 0.00141942064511 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 94 loss: 0.00139270587533 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 95 loss: 0.00136747489159 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 96 loss: 0.00134151639941 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 97 loss: 0.00131767499261 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 98 loss: 0.001293036883 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 99 loss: 0.00127038259234 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 100 loss: 0.0012473891984 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 101 loss: 0.00122525673942 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 102 loss: 0.00120378821157 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 103 loss: 0.00118288855447 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 104 loss: 0.00116264233657 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 105 loss: 0.0011424797849 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 106 loss: 0.00112273211562 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 107 loss: 0.00110409360059 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 108 loss: 0.00108595496567 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 109 loss: 0.00106800792128 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 110 loss: 0.00105011632695 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 111 loss: 0.00103324633528 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 112 loss: 0.00101702316897 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 113 loss: 0.00100177691638 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 114 loss: 0.000985459228104 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 115 loss: 0.000970226014033 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 116 loss: 0.00095512009284 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 117 loss: 0.000940689489653 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 118 loss: 0.000926219850953 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 119 loss: 0.000912161602173 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 120 loss: 0.000898533697182 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 121 loss: 0.000884871660674 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 122 loss: 0.000872397904459 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 123 loss: 0.000859627041791 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 124 loss: 0.000847097959195 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 125 loss: 0.000834706050227 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 126 loss: 0.000822588226583 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 127 loss: 0.00081099083036 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 128 loss: 0.000799185931101 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 129 loss: 0.000788095341704 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 130 loss: 0.000777285524237 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 131 loss: 0.000766460929299 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 132 loss: 0.000755918190407 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 133 loss: 0.00074608635623 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 134 loss: 0.000739095841709 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 135 loss: 0.000733559034416 train_accu: 0.973485 valid_accu: 0.742424\n",
      "Epoch 136 loss: 0.000718682014849 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 137 loss: 0.000709840656782 train_accu: 0.974747 valid_accu: 0.747475\n",
      "Epoch 138 loss: 0.000699795935361 train_accu: 0.974747 valid_accu: 0.742424\n",
      "Epoch 139 loss: 0.000690773238603 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 140 loss: 0.000682646154019 train_accu: 0.974747 valid_accu: 0.732323\n",
      "Epoch 141 loss: 0.000673506467137 train_accu: 0.974747 valid_accu: 0.737374\n",
      "Epoch 142 loss: 0.000663499355142 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 143 loss: 0.000656036558212 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 144 loss: 0.00064340854442 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 145 loss: 0.000635456075543 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 146 loss: 0.000627563982562 train_accu: 0.977273 valid_accu: 0.737374\n",
      "Epoch 147 loss: 0.000619432765234 train_accu: 0.977273 valid_accu: 0.737374\n",
      "Epoch 148 loss: 0.000611570540059 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 149 loss: 0.000603057414992 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 150 loss: 0.000595528210397 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 151 loss: 0.000588309587329 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 152 loss: 0.000580933232413 train_accu: 0.977273 valid_accu: 0.737374\n",
      "Epoch 153 loss: 0.000573416447878 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 154 loss: 0.000566490409255 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 155 loss: 0.000559396557946 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 156 loss: 0.000552599278308 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 157 loss: 0.000545504142792 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 158 loss: 0.000539463551831 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 159 loss: 0.000533210710273 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 160 loss: 0.00052648979181 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 161 loss: 0.000520080866409 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 162 loss: 0.000513906426931 train_accu: 0.97601 valid_accu: 0.727273\n",
      "Epoch 163 loss: 0.000507943208504 train_accu: 0.97601 valid_accu: 0.727273\n",
      "Epoch 164 loss: 0.000501861737575 train_accu: 0.97601 valid_accu: 0.737374\n",
      "Epoch 165 loss: 0.000495183227031 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 166 loss: 0.000490551447001 train_accu: 0.97601 valid_accu: 0.732323\n",
      "Epoch 167 loss: 0.000483889642055 train_accu: 0.977273 valid_accu: 0.727273\n",
      "Epoch 168 loss: 0.000478142974316 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 169 loss: 0.00047268970593 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 170 loss: 0.000467690300866 train_accu: 0.977273 valid_accu: 0.737374\n",
      "Epoch 171 loss: 0.000462806747237 train_accu: 0.977273 valid_accu: 0.737374\n",
      "Epoch 172 loss: 0.000457383008325 train_accu: 0.977273 valid_accu: 0.737374\n",
      "Epoch 173 loss: 0.000451707728644 train_accu: 0.977273 valid_accu: 0.737374\n",
      "Epoch 174 loss: 0.000446243633633 train_accu: 0.977273 valid_accu: 0.732323\n",
      "Epoch 175 loss: 0.000441566975496 train_accu: 0.977273 valid_accu: 0.737374\n",
      "Epoch 176 loss: 0.000437496109953 train_accu: 0.978535 valid_accu: 0.737374\n",
      "Epoch 177 loss: 0.000432633758464 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 178 loss: 0.000427406524977 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 179 loss: 0.000422182958573 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 180 loss: 0.000417712613853 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 181 loss: 0.000413164827478 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 182 loss: 0.000408994714235 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 183 loss: 0.00040463883488 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 184 loss: 0.000400611726945 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 185 loss: 0.000395787235902 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 186 loss: 0.000391364494135 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 187 loss: 0.000387208583561 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 188 loss: 0.000383276958019 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 189 loss: 0.000379112716473 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 190 loss: 0.000375551393518 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 191 loss: 0.000371716367226 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 192 loss: 0.000368188651919 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 193 loss: 0.000364132749382 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 194 loss: 0.000359962570656 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 195 loss: 0.000356226111762 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 196 loss: 0.000353019011527 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 197 loss: 0.000349459038262 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 198 loss: 0.000346003856976 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 199 loss: 0.000342148243362 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 200 loss: 0.000338732399541 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 201 loss: 0.000335304579494 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 202 loss: 0.000331903091137 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 203 loss: 0.000328632671881 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 204 loss: 0.000325793746015 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 205 loss: 0.000322530406265 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 206 loss: 0.00031909302561 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 207 loss: 0.000315903547744 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 208 loss: 0.000312934695103 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 209 loss: 0.000310070699925 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 210 loss: 0.000307303176669 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 211 loss: 0.000304231696646 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 212 loss: 0.000301185251374 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 213 loss: 0.000298414093777 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 214 loss: 0.000295390331303 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 215 loss: 0.000292819418974 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 216 loss: 0.000290247377052 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 217 loss: 0.000287862474579 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 218 loss: 0.000284835050479 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 219 loss: 0.000282004273686 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 220 loss: 0.000279234298432 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 221 loss: 0.000277147324596 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 222 loss: 0.000274551433904 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 223 loss: 0.000272267774562 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 224 loss: 0.000269557367574 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 225 loss: 0.000266894609013 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 226 loss: 0.000264646731011 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 227 loss: 0.000262090083197 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 228 loss: 0.000259933909547 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 229 loss: 0.000257868387052 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 230 loss: 0.000255285496678 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 231 loss: 0.000253134083323 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 232 loss: 0.000250731189226 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 233 loss: 0.000248428379564 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 234 loss: 0.000246365212661 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 235 loss: 0.00024399326503 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 236 loss: 0.000242253077886 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 237 loss: 0.000240236391619 train_accu: 0.979798 valid_accu: 0.742424\n",
      "Epoch 238 loss: 0.000238205366259 train_accu: 0.979798 valid_accu: 0.742424\n",
      "Epoch 239 loss: 0.0002360562994 train_accu: 0.979798 valid_accu: 0.742424\n",
      "Epoch 240 loss: 0.000233753497014 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 241 loss: 0.000231851150602 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 242 loss: 0.000229930981732 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 243 loss: 0.000227971480854 train_accu: 0.979798 valid_accu: 0.742424\n",
      "Epoch 244 loss: 0.000226131134696 train_accu: 0.979798 valid_accu: 0.742424\n",
      "Epoch 245 loss: 0.000224115572564 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 246 loss: 0.000222127404413 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 247 loss: 0.000220177378651 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 248 loss: 0.000218452658373 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 249 loss: 0.000216584905502 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 250 loss: 0.000215030633626 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 251 loss: 0.000213167624679 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 252 loss: 0.000211253351154 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 253 loss: 0.00020953096282 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 254 loss: 0.000207756176678 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 255 loss: 0.000206120845178 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 256 loss: 0.000204659541851 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 257 loss: 0.000202897823328 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 258 loss: 0.00020113726714 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 259 loss: 0.000199311220058 train_accu: 0.977273 valid_accu: 0.742424\n",
      "Epoch 260 loss: 0.000197467257749 train_accu: 0.978535 valid_accu: 0.742424\n",
      "Epoch 261 loss: 0.00019582711866 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 262 loss: 0.000194265681785 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 263 loss: 0.000192739969862 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 264 loss: 0.000190838778508 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 265 loss: 0.00018918670321 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 266 loss: 0.000187631207154 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 267 loss: 0.000186169871085 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 268 loss: 0.000184818200069 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 269 loss: 0.000182942021638 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 270 loss: 0.000181595065442 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 271 loss: 0.000179932281753 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 272 loss: 0.000178481672265 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 273 loss: 0.00017704057791 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 274 loss: 0.000175858156581 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 275 loss: 0.000174394415808 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 276 loss: 0.000172780477442 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 277 loss: 0.000171339372173 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 278 loss: 0.000169825561898 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 279 loss: 0.000168662203578 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 280 loss: 0.000167336736922 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 281 loss: 0.000165825291333 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 282 loss: 0.000164434255566 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 283 loss: 0.000163044398505 train_accu: 0.978535 valid_accu: 0.752525\n",
      "Epoch 284 loss: 0.000161777325047 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 285 loss: 0.000160584171681 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 286 loss: 0.000159351664479 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 287 loss: 0.000158346814715 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 288 loss: 0.000156824657097 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 289 loss: 0.000155551610078 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 290 loss: 0.000154425168148 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 291 loss: 0.00015325824279 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 292 loss: 0.000152172344315 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 293 loss: 0.000151056650793 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 294 loss: 0.000149800282998 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 295 loss: 0.000148689348862 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 296 loss: 0.000147734572238 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 297 loss: 0.000145867861647 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 298 loss: 0.000147349202052 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 299 loss: 0.000144932232615 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 300 loss: 0.000144365954839 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 301 loss: 0.000141976035593 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 302 loss: 0.000141033180626 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 303 loss: 0.000140065261803 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 304 loss: 0.00013894121912 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 305 loss: 0.000137864840326 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 306 loss: 0.000136601311169 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 307 loss: 0.00013550465701 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 308 loss: 0.000134336501105 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 309 loss: 0.000133282762363 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 310 loss: 0.000132431684506 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 311 loss: 0.000131515035719 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 312 loss: 0.000130380241899 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 313 loss: 0.00012947193045 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 314 loss: 0.000128533829411 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 315 loss: 0.000127532536681 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 316 loss: 0.000126700513647 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 317 loss: 0.000125935265714 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 318 loss: 0.000124945891002 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 319 loss: 0.000123917187011 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 320 loss: 0.000123056555822 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 321 loss: 0.000122154200653 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 322 loss: 0.000121433030472 train_accu: 0.978535 valid_accu: 0.747475\n",
      "Epoch 323 loss: 0.000120691626762 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 324 loss: 0.000119516293125 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 325 loss: 0.000118448248031 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 326 loss: 0.000117280056656 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 327 loss: 0.000116076119411 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 328 loss: 0.000115149929115 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 329 loss: 0.000114179620141 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 330 loss: 0.000113156866973 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 331 loss: 0.000112216357593 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 332 loss: 0.000111414133244 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 333 loss: 0.000110561832116 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 334 loss: 0.000109792988042 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 335 loss: 0.000108921624815 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 336 loss: 0.000108180188363 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 337 loss: 0.000107366047814 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 338 loss: 0.00010648513944 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 339 loss: 0.000105907001853 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 340 loss: 0.00010540994117 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 341 loss: 0.000104637517325 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 342 loss: 0.00010382455639 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 343 loss: 0.000102786299976 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 344 loss: 0.000102266574686 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 345 loss: 0.000101627656477 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 346 loss: 0.000101142507447 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 347 loss: 0.000100429678241 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 348 loss: 9.94855936369e-05 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 349 loss: 9.99694620987e-05 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 350 loss: 9.84556945696e-05 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 351 loss: 9.85367760222e-05 train_accu: 0.97601 valid_accu: 0.742424\n",
      "Epoch 352 loss: 9.74949352894e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 353 loss: 9.65544095379e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 354 loss: 9.57509791988e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 355 loss: 9.48486094785e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 356 loss: 9.41524613154e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 357 loss: 9.35624057092e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 358 loss: 9.27613746171e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 359 loss: 9.18018295124e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 360 loss: 9.02891488295e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 361 loss: 8.88170006874e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 362 loss: 8.76738331499e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 363 loss: 8.68370207172e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 364 loss: 8.60764976096e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 365 loss: 8.53255160109e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 366 loss: 8.44791784402e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 367 loss: 8.35636947158e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 368 loss: 8.28854190331e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 369 loss: 8.2072450823e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 370 loss: 8.15300754766e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 371 loss: 8.07874394013e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 372 loss: 8.02545873739e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 373 loss: 7.95262540123e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 374 loss: 7.89850728324e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 375 loss: 7.83485093052e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 376 loss: 7.78740832175e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 377 loss: 7.734124938e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 378 loss: 7.67738329159e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 379 loss: 7.62219187891e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 380 loss: 7.56282788643e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 381 loss: 7.51216575736e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 382 loss: 7.46961013647e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 383 loss: 7.41441908758e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 384 loss: 7.37484315323e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 385 loss: 7.32883063392e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 386 loss: 7.2834137427e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 387 loss: 7.22095037418e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 388 loss: 7.19412910257e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 389 loss: 7.14322941349e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 390 loss: 7.11879338269e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 391 loss: 7.05203842699e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 392 loss: 6.99732290741e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 393 loss: 6.9629918471e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 394 loss: 6.91638347234e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 395 loss: 6.87382744218e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 396 loss: 6.83031794324e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 397 loss: 6.78311253068e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 398 loss: 6.7440134444e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 399 loss: 6.70002714287e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 400 loss: 6.65902052788e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 401 loss: 6.62325905978e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 402 loss: 6.58618710077e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 403 loss: 6.53945871818e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 404 loss: 6.49618687021e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 405 loss: 6.45959125904e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 406 loss: 6.42084987703e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 407 loss: 6.38508840893e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 408 loss: 6.34384332443e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 409 loss: 6.30712829661e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 410 loss: 6.27410854577e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 411 loss: 6.24537997282e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 412 loss: 6.20008231635e-05 train_accu: 0.97601 valid_accu: 0.757576\n",
      "Epoch 413 loss: 6.16551296844e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 414 loss: 6.12581752648e-05 train_accu: 0.97601 valid_accu: 0.757576\n",
      "Epoch 415 loss: 6.08469167673e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 416 loss: 6.05310233368e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 417 loss: 6.01793713031e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 418 loss: 5.98217548031e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 419 loss: 5.94677162553e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 420 loss: 5.91065227127e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 421 loss: 5.87202966926e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 422 loss: 5.847115699e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 423 loss: 5.81314270676e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 424 loss: 5.78119575039e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 425 loss: 5.74424211663e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 426 loss: 5.70836109546e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 427 loss: 5.67808256164e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 428 loss: 5.64875845157e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 429 loss: 5.61263923373e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 430 loss: 5.57842727176e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 431 loss: 5.54636094421e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 432 loss: 5.51477137378e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 433 loss: 5.48902321498e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 434 loss: 5.45385746591e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 435 loss: 5.4215527598e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 436 loss: 5.3882942666e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 437 loss: 5.36040038241e-05 train_accu: 0.97601 valid_accu: 0.752525\n",
      "Epoch 438 loss: 5.33429433744e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 439 loss: 5.30687716491e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 440 loss: 5.27075803802e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 441 loss: 5.23881067238e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 442 loss: 5.21473102708e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 443 loss: 5.18802944498e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 444 loss: 5.15930055371e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 445 loss: 5.13009504175e-05 train_accu: 0.97601 valid_accu: 0.747475\n",
      "Epoch 446 loss: 5.10029381076e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 447 loss: 5.07430677317e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 448 loss: 5.04390973219e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 449 loss: 5.02030693497e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 450 loss: 4.99169746035e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 451 loss: 4.95510116707e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 452 loss: 4.93304801239e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 453 loss: 4.91230616717e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 454 loss: 4.88774990117e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 455 loss: 4.85747168568e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 456 loss: 4.8295776196e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 457 loss: 4.79953769172e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 458 loss: 4.77664993923e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 459 loss: 4.74911344099e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 460 loss: 4.72908695883e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 461 loss: 4.70190789201e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 462 loss: 4.67639783892e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 463 loss: 4.65362945761e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 464 loss: 4.62752341264e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 465 loss: 4.6053512051e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 466 loss: 4.57507280771e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 467 loss: 4.54920486845e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 468 loss: 4.5292975301e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 469 loss: 4.50378756796e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 470 loss: 4.48256873824e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 471 loss: 4.45944288003e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 472 loss: 4.43488638666e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 473 loss: 4.41307151959e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 474 loss: 4.38839583694e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 475 loss: 4.36622358393e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 476 loss: 4.33975978922e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 477 loss: 4.31973312516e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 478 loss: 4.29922952208e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 479 loss: 4.27288491665e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 480 loss: 4.25726902904e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 481 loss: 4.22961315962e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 482 loss: 4.2049372496e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 483 loss: 4.1872948259e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 484 loss: 4.16524148932e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 485 loss: 4.14461892433e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 486 loss: 4.12220824728e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 487 loss: 4.09610197494e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 488 loss: 4.07750567319e-05 train_accu: 0.977273 valid_accu: 0.747475\n",
      "Epoch 489 loss: 4.0567637825e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 490 loss: 4.03816770813e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 491 loss: 4.01229985982e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 492 loss: 3.99418036068e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 493 loss: 3.97737221647e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 494 loss: 3.95734559788e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 495 loss: 3.938511054e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 496 loss: 3.9137159547e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 497 loss: 3.90763657379e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 498 loss: 3.86090760003e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 499 loss: 3.84338427466e-05 train_accu: 0.977273 valid_accu: 0.752525\n",
      "Epoch 500 loss: 3.82204634661e-05 train_accu: 0.977273 valid_accu: 0.752525\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90ad204438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841, 192)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841, 99)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.matrix(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841, 99)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot(t,99).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 194)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train)),df_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
